So today, we will start our part 3 and part 3 is focus it on, I would say, automating data ingestion processing unstructured data and ingesting data in real time. So it is very interesting part, so it is the largest part. So here we have 4, 4 or 5 classes. part one. We had 2 classes just about Hdfs and distributed storage and distributed processing. Part 2 was about data analysis, using creating the schema, using hive, querying, using Trino and selecting or choosing the best file format. So today. And until the end of the course, we'll be working with semi-structured data. Jason, maybe. Yeah, most commonly, Json, and we will automate everything. So we'll not orchestrate the task, but we will automate many, many tasks from ingestion to to visualization. There's some still some parts manual, because the orchestration is more, I say, if we will be using, for example, Apache airflow. it will be more focusing on development because we need to create the task using python or create the dag using python. So it was. It is more oriented development. So we will not be doing this in the class, as we are more focusing on the data engineering side. So this class, the topic of this class is very large. So I will split it into 2 class. So today and the next class will be talking about the same topic, how to ingest data at scale, how to automate all these small tasks. And you see it is very, very interesting, and it is the foundation of, say, any data, pipeline workflow. Okay, so now. just to to give you an idea, we'll be talking about 9, 5 Apache 9, 5, which is our automation tool and ingestion tool. So we finish, we completed the storage. We completed the data analysis, the data processing. And now this is the latest part data ingestion. Okay, so in here we add velocity. So here we combine volume, variety and velocity volume. You have now the idea and the foundation. And how can we use different storage system in our class? We are using Hdfs. But it is the same concept for other storage. So for example, if you are storing your data on Aws or on Google Cloud Storage or azure blob storage, the the concept is the same. It is transparent for you. You just need to create the structure of your on your on your device. Storage the Directory and give permission in in our class. We don't deal with permission sometime we need to deal with it, but it is. You are running as an administrator. So you have all the permission to do whatever you want. But in real world the administrator should grant you the permission to to write to read whatever all this permission should be granted. But the the foundation and fundamental are the are the same for the data processing. You can do exactly the same with Databricks, or if you run an Emr on on Aws or data, proc on Google, you can run spark. You can run Hadoop Hdfs exactly the same. Maybe you will not find all the tools we have in our sandbox, but it is exactly the same with Databricks. The the foundation are the same. We are dealing with spark Scala, wherever. But maybe you and you will use a customized jupyter very similar to what we are doing with Zeppelin. But they have, added more more keywords to, for example, to let you interact with the file system with their file system and show graphics and embed graphics in in the in the notebook. but the concept are the same. Now, this is our data process model, do you remember? And now you are comfortable with all these stages, so you can collect your data. For instance, it is manually you are doing just uploading data from from your local system to Hdfs. You are doing this using one or 2 commands, for now this will change, and once the data is on Hdfs, you can run your spark, for example, or mapreduce for for unstructured data. But most commonly you will run your spark code to organize, to prepare and to clean your data. Then you need, if you need to query this data interactively. You need to create the schema using hive and a query using trino, once the data and the query are are here. Now, you can run. If it is just a single chart, a very simple chart, you can run this into a Zeppelin node, Zeppelin cell, or you can build a full dashboard using Apache support. So now you have all these pieces together, you can just put this piece together and you are able to to create an end-to-end solution with data at rest. For instance, data is not moving and that it is the process is not automated. We need to do it manually, but this will change a little bit. So in this class, we'll end the next class. We'll be focusing on the collect side. So here you will be doing some some data ingestion, very light transformation, routing whatever. So we have many, many things to do here. And again, this is not where we will be doing our heavy transformation. What what I mean but heavy transformation. For example, you load your very large data set into spark and you you clean, you. You normalize the date you remove unwanted rows from your tables, and so on. You cannot do this here because it is not where we need to do it. We we need to do it. or we must do it in the organizing stage, not in the collect stage, but in this stage we can do some very light transformation. We can change data format. We can simplify the data by filtering. We can choose to route based on criteria and so on. So this is what we'll be doing here. So today, topic is about Apache Wi-fi. So we will talk about the ingestion. the data ingestion concept more into details and how to ingest large volume, and we will be talking especially around Ni-fi. What is Ni-fi? What is the data flow? What is? What is the terminology used in flow file in about flow files, connections, processor anything about this? Do you have any previous experience in Wifi? Did you heard about it? Or it's it is something new for you. New. Okay? Okay? Okay? So almost, there's something new. So you will be. This course is very, you will enjoy because you will do things differently using. Wi-fi, okay, great. So the question is for today, for the topic of today and for you how to transfer structured or unstructured data from any source to any destination until now. What you did. You take a file from the local system, you upload it to Hdfs. Here we are defining the source as local system and the target as Hdfs. But in real world this is not enough. So we we have different sources. We can maybe I need to ingest data from a database, a table. and do something with it with another system outside the Rdbms. Maybe I have receiving a file. I should connect to a rest Api somewhere in the Internet. And I collect the data from there. And this is also a different data source. the destination. Maybe I need to upload my data file from local system, but not to Hdfs, maybe to create a hive table or whatever so, or push the data into Kafka topic, which will be our class, not the next, but 3 classes. So all these are in real world scenario. So we need to connect from different from different any source, any source to any target or any destination. So how can we do this? And we want also to connect not only structural data we want to use unstructured data in the course, we will be focusing on semi-structured data, such as Json. because the most common scenario we receive Json data from a vendor or from endpoint somewhere on the Internet. Or we receive this whatever. And we need to process this Json file. So this is the most common scenario. Some other systems, we can maybe need to do the same with Xml. So the idea is very similar. But working with Json is easier than working with Xml, so this is why I choose to work with Json file instead of working with Xml file. No, the data when you upload your data or your file on Hdfs. The data is not moving. The data here is stored as a file on the local system. You just run the command, put or copy from local to upload from the local system to the target on Hdfs. But if the data is moving, the data is, for example, I am collecting data from an endpoint on the Internet. And the data is changing. So it is not the same process. It is not so. Here we have the velocity. and if the velocity is low. so we can maybe handle it easily. But when it's come to very high velocity or high velocity to very high velocity, things are not the same, we cannot handle it easily. So here, where our data ingestion system will help us to do this. So we will learn what is data ingestion system, a good data ingestion system. And how can we transfer from any source to any destination? And how easy to scale? This is very important. And again, the data ingestion. It is not where we will be doing data transformation or heavy transformation. So here we need just to connect to the source, collect the data and transfer it. This is all we will be doing in this part of the process. Okay, so let's start with what is a data ingestion. So you should. I think you are very familiar with data ingestion. But this is just to put to the right definition. so that to ingest something is to take something or to absorb something. So data ingestion means I will take the data from a particular source. and that ingestion process is import getting the data and importing for immediate storage or use. The destination or the target might be a storage device or not. Maybe I am collecting my data to put this into a table in in whatever rdbms or not. Necessarily as a file on Hdfs it could be. Maybe if it is a file we can maybe put it on local system. Maybe if it is an event, we can just push this into Kafka topic. If it is a Json file, maybe I want to process the Json file and send it to a table in my rdbms. So you so you see, the target is not always storing a file. It is not always a storage device. Okay, so the data can be ingested in different ways. So it can be ingested in real time. So this means the data is changing over time and is coming and changing over time. We will learn the difference between real time and near real time when we'll be talking about spark streaming spark streaming is Class 9, 9 or 10 class 9 or 10. So we I will explain the difference between ingesting data in real time or ingesting data in near real time. But, for instance, we'll be talking about real time. So real time. So this is what we call streamed data, or it can be ingested in batch in batch. This mean, the data is not ingested whenever it comes in our system, it is stored somewhere. and I connect once, maybe based on some criteria basing, maybe on our criteria or every hour every week whatever. So, and I will take all the existing data and process in one shot. So we can do this. And we have different example. For example, if you want to process streaming data, understand as your broadcasting Channel and your TV station. The broadcasting channel is always broadcasting the data, the movies, or whatever. And whenever you turn on your TV, you connect to this channel. And you see the movies. So streamed data is, this is the example. So whenever the source is always emitting or broadcasting the data. and whenever you connect to the channel, you can get this information, and when you disconnect from the channel you you cut this relation between you and the broadcasting machine in batch again. You, as I said, you can store this somewhere and decide to process this every hour, every 6 h, every day, every week, every month, whatever. This is what we call batch processing. Okay? So now let's see, the the data ingestion process is very important. Why? Because from my perspective, it is a single point of failure. because the data ingestion is the precursor to any analysis. If you don't have the data, you cannot process your data, you cannot create visualization. You cannot do anything. So this is very crucial. and it is a single point of failure. You don't have the data, you cannot move forward. So this is why the data adjusting process is very important process. When you start your data analysis or data engineering again, data ingestion is about to deliver data from a source to a target or a destination. We can define wherever we want the source and wherever we want the target or the destination. So the main characteristics of a good data ingestion system is to be reliable. The correctness, the speed, and the scalability real means. I want to to rely on the system on this data ingestion system that my data will be delivered in case of failure. Of course, if I am ingesting data at very large scale, and in case of failure, I want to ensure that I will always get my data in case of overload. Maybe I will ingest a lot of data. But my, the drain or the system, who is draining this data out from the ingestion system is not fast enough to to drain all the data. I should not lose any data. So this is why this is the transition system should be reliable. The correctness means the data should not be altered should not be altered when it is collected from the source and delivered to the destination, should not be altered. But if you decide to do some transformation, it is different, but in case of you don't apply any transformation, the data should should be correct, should be delivered without to be altered. The speed and scalability are correlated. So this means the system should be fast enough to not lose any any data, of course. And in case of, I need to to ingest more data or larger size of or volume, I should be able to scale out my data ingestation system. So the main characteristics are readability, correctness. speed, and scalability. So now, this is just to illustrate what I said. We have a lot of data structured or unstructured. I can ingest this in batch, or in real time or near real time, my system should deliver either unstructured or structured data without any alteration should be corrected. Correct. No, not no data altered. If I want to process this in batch or in real time, the system should be able to deliver either. both if it is to be processed in real time. The Sys data ingestion system should be able to deliver it. And this is the case, for example, when you are ingesting events. So we'll talk about even to the next class. So events are real. We say real time data. So it should be processed as soon as it is ingested and batch. This is, for example, you collect and you store on Hdfs. You will process. This later could be a minute after could be hour after whatever. But this is the batch processing, so the system should be ingestion system should be able to ingest your data wherever the form is structured or unscriptural, and deliver in batch mode, or in real time? No, the data ingestion. So we we know now that we we should have a source and a destination. This is Montato to to ingest something. So I need to connect to my source. Once I am connecting to the source to extract the data, I need to deliver it somewhere. So the connect and deliver or store if you want or Montato. Now, between these 2 steps or 2 phases, I have 2 optional steps. This is about transformation and routing. Maybe you need to transform your data. Maybe you need to change your date format. Maybe you need to change the file format. Maybe you need to simplify because you need to remove unwanted rows. For example, blank, empty rows. You need to remove this. So this can be done here here in this process, within this transfer from the source to the destination. But it is optional. Not always. You need to do filtering. Not always. You need to do that date normalization and so on. So transform is optional. But it can be done and routing also, you maybe you need to route based on criteria. So, for example, you are processing financial data. And you are, you want to separate the currency Canadian dollar, us dollar and euro to different directories because you want to process them differently. So you can route this here based on the criteria currency. Maybe you want to route, base it on content, on volume or so you can decide whatever it is. So routing, again, is not mandatory, is an optional step it. But in real world scenario we always do these 4 steps in in, because the data is not always as you want it to be. So you need to do filtering, maybe to remove empty rows, maybe to simplify because you have a lot of information, for example, in a Json file. And you need to simplify this. So maybe I would say in every scenario we need to do some transformation like transformation routing, not necessarily routing. But whenever you get a failure, you need to inform, for example, the administrator, maybe to to process this later. So here we can use routing. So routing, for example, if you are processing financial transaction, and you do. For some reason you suspect this is a fraud or whatever. So you can just route to this, this event, or this piece of data to another system to to be processed, and check if it is a fraud or not so the the application are really, really, very, very large. So but connect step is mandatory. Transform is optional. Route is optional and store or deliver is mandatory. So this is our data ingestion process, and whenever the data is delivered. the data addition process terminates here. So, for example, I'm connecting to an endpoint on the Internet and collect the data. and the process should deliver the data to be stored on Hdfs in parquet format. This is my ingestion process. So I connect to to the data source I convert. So I transform the input data into parquet. Maybe I do some partitioning, and I store this on Hdfs. Here. Once it is stored, the the transition process is finished is considered as terminated. The task finish here. Now, you need to run your spark code to process the parquet file, for example, stored by your data ingestion process, you see. So here we start another step, which is data organization using, for example, spark. Okay, maybe you need to do some very heavy transformation here, and then you move. When it is finished, you move forward in creating schema and query this using Trino. So the data ingestion process starts by connecting to the data source. doing some light transformation if needed, some routing if needed, delivered the data, and the process is finished here. Finish here. So there is many, many popular tool to ingest your data. Many of them are now legacy, but I found some of them still are using in some legacy project. but just to let you know that exist. But it is known, flume, scope. stream set was created by Cloudera at 9 5 was bought by Cloudera in 2,018, and since they bought Apache Wi-fi, and now they they don't, I would say, continue developing these legacy. Tools, flume, scoop, and stream set. So flume and scoop are 2 lockstash and goblin. All these are command line tools. So flume is data ingestion tool, very efficient. But base it on text file. So you need to create parameters for the input for the output. It is somehow limited, but it it does the job scope was created to ingest data from any database and collect to say, import export from any database to Hdfs and back from Hdfs to any database. So now it again you can choose your format sequence, file avro or parquet, or leave it in text. It is also command, line, tool it is. It was very efficient. But now it is not. No, no more. We don't need it anymore, because we can do everything with 9 file log stash is very similar to file. It was part of the elk, stack, elasticsearch, log stash, and kibana and log stash now has been removed from this stack and embedded into elasticsearch. Goblin is very similar to flu. So it was a data ingestion tool based on text file to define the input and the output and was developed by linked in stream set, is the only tool we had that have a visual interface, and you can connect visually your component to create your data flow. Since Ni-fi was bought by Cloudera and stream set was developed by Cloudera. So Cloudera decide to stop developing stream sets. This is why now every everything is, and almost all the project I worked on. All the company that are using knife is very powerful tool, and we have different also tools nowadays using. So we have Kafka connect. We have Kafka streaming different version of Kafka. We have spark streaming, we have Flink, we have Apache Euron. We have a lot of tools to ingest and process data. The main difference is if it is orchestrated automatically or not, and if it is supporting real time or not, and what is the use case you will be using? So with my file, you will be able to build a data flow. So this mean, you can build visually all the steps to ingest. to transform light, transform, to route and to deliver your data from the source to the destination. Great. So do not hesitate if you have any question. So what is Apache? Ni-fi Ni. 5, stand for Niagara files, not Niagara Falls, Niagara files, which is an open source. Data flow that collect route and reach transform and process your data. What is enriching, enriching means. I have a data from coming from a source. I have another data coming from another source. I can combine them together. Let me give you an example when the administrator process the log file for a web website in the log file, the administrator can see the IP address IP, this IP address connected to our web server at this time, so he can enrich this by adding the country or the geographical information or location. For this IP address. So you can combine this with the the database. Open database where to get the location from IP address. And this is what we call data enriching. So it, we can enrich our data. We can combine data from different sources to produce a new data. So with Ni-fi, you can collect, you can route, you can handle it, you can transform. And you can process your data in a scalable manner. So this is, if you here you can see that knife offer us, or all the features needed by, or should be in a good ingestion system. It's it has all these features. So what is important that Apache Ni-fi is the processing engine based on the concept of flow-based programming model. What does this mean? The flow basic programming model is a programming model. It is not a programming language, it is a programming model. So the Pro. This programming model say, everything is executed sequentially in in the sequence and every operation. Any operation should apply on input data and produce output data. So the flow basic programming model means any task should be executed sequentially and any operation, any operation should apply an input and produce an output data model. So if you don't want or you don't need the output data, you need to explicitly say, I want to stop the process here. the the flow the the flow programming model, the flow. I want to stop here. So any operation, any operation should apply or operate on a piece of data and will produce a data. produce an output. If you want to stop this flow, you need to say explicitly. I want to stop the flow here. This is what is the flow basic programming model or Fbp. very simple. But just understand that any operation operate on data and will produce data. If you want to stop the flow, you need to explicitly put the the ends of this flow. Okay. So Apache Wi-fi has. Everything is visual, so you have a canva, and you will put everything on this canva. Everything is drag and drop, so it's visual. It is very comfortable and very friendly user. So it provides also data lineage. So this is very important in an enterprise environment. Why? Because when you process your data, you know exactly from where you get the data and what all the steps the data processing was. And you can just create this data lineage. So this is very important, and especially when you are doing data governance. So Apache also can prioritize your data. If you are collecting or ingesting data from a particular source. Let's say you are collecting data from a sensor. And for some reason the sensor sent some data delayed. Okay, so here you can base it on particular criteria. You can say, if I got this. create this information in the flow file. just prioritize and let this data go first.st So you you can prioritize your data. You have different algorithm and different ways to prioritize your data. Also, it provides data buffering and back pressure. This is very important, and especially when you go at scale. What this mean, you are ingesting data. and the system is delivering your data to the target. So let's say, the data source is a real Twitter information. The Twitter platform or X platform. and your data ingestion is the target is Hdfs. So if you collect one tweet per minute. It is not a problem, but when it comes to collect 10,000 per minute, it's a problem. So Hdfs, for example, is not fast enough to drain all the data collected by Nifi. So this is where Ni-fi will buffer your data so it will act as a buffer, and it will store the data ingested data into this buffer. Now, when the buffer is full, because your ingestion, the drain is not fast enough to collect all this data or extract all this data from the the buffer. So now, if I will enable or trigger the back pressure mechanism, this means it will ask the source to slow down until the buffer is flushed or empty, or getting some threshold before collecting new data. So this is, can this mechanism is enabled automatically by an ifi, and it is very helpful, and especially when you go very fast, when you ingest very fast data, very high velocity, and your draining system is not fast enough to extract everything from the from the data flow. So also we can control the latency and of your data flow. For example, I want to connect to a data source on the Internet. I want to connect to an Api, for example, I want to connect to the flight rather to get the information. It was one of my capstone project prior session. We need to connect to the flight radar and get the information about the flight. But as we are using the free plan, so we are not allowed to. We, we have a limited number of query. So here you can control this querying. How many times, how fast you need to query, to need to connect to the data source to get the information. So you can control everything here. And also we can control the throughput. So you can control both. Latency is throughputs and we can control everything. So it is very secure, but in our so you can choose. The administrator can choose who and grant who has access to this platform. But in our platform. You are administrator of the of the system. So you can. You can do whatever you want in this platform and scale out and extensible scale out means, I can create an I-fi cluster. So this means I can create many, many nodes and use these nodes to collect. And ingest data. And when I ingest some data, I can decide where to on which node I can ingest or run this particular, a particular task. extensible means by default, it provide you many, many features and many operations that you can use to process your data, to ingest your data, to connect to a different source and a different target. But if it is not enough, you can create your own Plugin, and it is created in Java. Usually it is created in in Java. It is possible in one of my previous session, not for Mcgill, for another training center. I had the exercise to to let the to ask the student to create an extension for 9 5. But it was development course, not the data engineering course. Okay, so what is the Apache? Ni-fi. Now, you have an idea about the ingestion system. What is Apache? Ni-fi features are. So apology is used to collect your data to enrich your data and to deliver your data, you can use it again to do some light transformation, to convert between format, to extract and parse some some data and route based on a criteria. The criteria can be an attribute that you provide. or it can be extracted from the data content the content from the data. So you can decide. This is, again, our data transition system should be very. It is very efficient, and you should apply light transformation and routing if needed. But it is not a distributed computation engine. It is not where it will not replace your spark. You will not run all the Scala code you will be doing in spark to process your data. It is not a computation engine. You can. You cannot do the same. What you can do with a spark. Again. It is very important. It is very light transformation. It is not a heavy transformation, it is not a complete, it provide you some features, some command that will let you access your data and do some transformation, some extraction. For example, you can extract attribute from a Json, you can split a string, you can add string to a path. And so so this very light manipulation and will hold your data to keep the data provenance. But again, this is not used at our level. This is used when you are doing this in for an enterprise environment, and especially when we need to do governance data governance. And it is not that document indexer, so it will not index the content of your data. It will not do all these tasks. You do not have these capabilities. so if you need to index a document, you need to use another tool, such as solar or elastic search. So these tools are dedicated to index. When we say, indexing your document. This mean, you can search. You can run a query to extract insight from this document. This document can be a text file can be a table can be a Pdf. Whatever and you have. If it is a Pdf, you need to use ticker Apache ticker. If it is text you can use Apache solar or Apache elasticsearch. So all these are here to index your document. But it is not the way where you will be doing this. Infi is here to ingest your data, apply light transformation and routing if needed. Okay, so my file is Java basic. So let's start from the bottom of this architecture overview. It is the local store, so everything is stored locally on your sandbox, of course. And now, if I provide 3 repository. the 1st one is the Flow file repository, the Content repository, and the Provenance Repository. The Provenance Repository is for data lineage. So it is not for us the content repository and the flow file repository. So the content is your data content. So whenever you collect your data. remember, the good ingestion system should be reliable in case of failure. You should not lose any data. so my file will store your data locally until you extracted yours. You you deliver to to Hdfs. You deliver it to Kafka. You do whatever you do with your data until it is extracted or delivered, it will stay here. So this is the role of the content representative. The Flow file repository is what you are creating as flow, file, and flow, file is the unit. You will see. I will explain this in a moment the unit used by an Ifi to transfer data. So this is the where Wi-fi will store your data information and Meta information and the content of your data. On top of these repositories we have the flow controller. So here you are creating a dag. you are creating a flow file. So the workflow. So you start by connecting to a data source, you do some transformation. So everything is splitted into operation. The component responsible to process this or to accomplish this operation is called processor. So in a data flow, you can create and add as many processor you need. And of course you can also add your a customized processor. It is an extension. So all these processors are controlled by the flow controller. The Flow Controller is responsible to start, to stop, to enable to disable all these processors so and the processor will operate on data. The data is stored in the Content Repository. You see the relationship now and how you interact with Ni-fi, you interact with Ni-fi using the web browser, your web browser. So now, if I provide a web server, so you can connect to this web server using your web browser, and you'll get your interface from your web browser. Everything is hosted on your single machine. You see, it is very. The architecture is simple. but just. We need to understand about this new terminology, the flow file and the processor. I will explain this in a moment. So this is a standalone. This is how it is installed on your sandbox. It is in standalone mode. You can also create a cluster. But here each node should be one physical machine. It doesn't make sense to install this on a single machine. And here we need to connect to zookeeper, which is the server coordinator, a configuration coordinator and cluster coordinator. So in case of failure. it will elect a new lead. And so it is, I will say, very common architecture, using zookeeper the same. They all use zookeeper for this purpose. But again, it is not our infrastructure. But here, in this configuration. when you run the data flow, you can decide on which node to run this processor or that processor, and so on. So the terminal terminology you need to know in. If I are the flow file processor connection and process group, these are very important terminology. So the flow file is the unit of data moving through an Ifi. when you connect to a data source, let's say you need to, or you want to read the file from your local system once. Now, if I read the file, it is converted into flow file. So inside my file, the data is store it into structure called phlophyde. So it is a unit of data moving through an I. 5. If you need to do transformation, if you do whatever you need to do, it will be by acting. operating on this flow files. So the flow file has 2 parts, the contents and attributes the contents is your data. the data. If you load it from a file, you get it from memory whatever. So is this your data. and the attribute is a collection of K per value. So K, per. I will explain this in a moment. Okay, so this is how the data is transferred or moving through knife. Now the processor is the action. Perform it on the profile. So the processor performed the work and can access your flow files. For example, I want to change the date format. Okay? So your input data is converted into let's say, convert, Jason, to approve or to parquet. So your input is Json, the Json is converted into flow file. You use the processor dedicated processor to convert into parquet. It will take all these flow files, convert into parquet and store on disk, for example. so the the processor will perform the task or the operation you want to perform. The connection links 2 processes together. You want to connect for, get file, to read the file from your local system and to read the file, or to split it, or whatever. So you need to connect or to link this task or this operation together. This is where you will be using the connection. Okay, so the connection is the link between the processor. And this is where Ni-fi will create the queue for you. So the queue is created. This is where the buffer will will be created. This is where the back pressure will be created or triggered whenever needed, and this is where you will prioritize your data flow. You prioritize your data flow in within this connection, because the the data is outputted from the processor. And now you can operate and prioritize, do whatever you want to do with this data. The process group is nothing but a way to organize your data flow in Ni-fi. We don't have the concept of a new data flow. Everything is stored on the canva. There is a main canva. Everything is here, so there is. We don't see we don't have create a new data flow. You give a name for this. Data. Flow doesn't exist. So everything is on the canva. This is where why you need to organize your data flows into process group. So the process group is a kind. Let's say it kind of database in your database in your rdbms. So you create a database and you add tables to this database. So the process group is namespace. Use it to separate and simplify the management and on the illustration you can see that now, when you create process groups. for example, you want. And this is how you will organize your work in my file. For example, we need to ingest data and do very light transformation and store it on Hdfs. Let's say we have a Json file. We connect to the data source. We do some transformation we simplify, we remove unwanted attributes from the Json file and store this on Hdfs in parquet format. Very simple data flow. I here, I will create a main process group inside, I would say, data ingestion data processing data storage. So I create 3 process group process group can be embedded. It is kind of directory namespace. and inside I will put all the processor, for example, needed to connect to the source and get the Json file. In the second process group. I will put the Json transformation to keep only the attributes I want in the 3rd process group. I will store. I will process the Json to convert into parquet and store it on Hdfs. Even we have one processor. It doesn't matter. But this will organize your data flow. So when you connect process group together, you cannot create a connection. Connection works only when you connect processor together to connect process groups together, you need to create input port and output port. So the output port is here to output the dot flow files from a process group. and the input port is here to collect the data from another process group. So connections are for processor input output ports are for process groups. Again, you will be more comfortable with this in the workshop, because you will see how it's work. You'll see how to connect this. So it will be. You will be more comfortable with this. Okay? So you can see in this illustration in the yellow area. This is nothing but a yellow rectangle, just a label, nothing special here. And you can see processor connected together. And you can see this link between the processor. We have a kind of a box. This box here has all the information you need to prioritize, to define your threshold, to define your back pressure. Everything is here. In this box. In the outside the yellow area you see 3 process groups connected together. And if you if you see the con, you can see in the connection, you can see also a red square. This is this mean? This connection is stopped. You will see this in a moment in more bigger size. Okay, so the 9 5 flow file, which is a very important piece of flow of 9 5. It has 2 parts. The header contains all the attributes, attributes is nothing but a k per UK. Pair values. for example, the decoration date, for example, the file name for so you have. The K is file name and the value. For example, you have last modified. so last modified attribute, we have the value. So in the header we have the collection of attributes. You can add your custom attributes, or you can use the embedded or default one. Okay. the second part of the flow file is the content. So this is the your your data. But again, it will, it will not. For example, if you load a file and this file is, I don't know. Maybe 10 MB or 20 MB. Maybe it needs more than one flow file to be to be loaded so. But again, no, if I will try to load everything in a single flow file when you load, for example, a file, if it is very large, should be splitted. Good. Now you can access this attribute and this content, using the Ni-fi expression language. So now, if I provide an expression language, let you, accessing this attribute and the content. So this is where you will be doing transformation. For example, you can read the content from, for example, a date from a content. And you say, Okay, I want to modify this to be this time format, for example. So 9, 5. Expression language is very, very important, very simple, but very important to understand, and you have a dedicated tutorial to let you practice this part. Okay. So for the header. as I said, here you have the K value pair. So here you have some default. Attribute not all of them, but some of them, for example, you have the file name. You have the path, the uuid, unique, unified id, the entry date, the lineage start date. So some of them you need to use it to use them. Other we don't need for our class, but we have a lot of default attribute already created and added for you, whenever you load a flow file or you create a flow, a flow file. So. for example, it can represent. If you say I want to split a text file and so, or a Json file, and you need to know how many fragments or how many element or the file size, you can access directly these attributes and get the information directly using the file expression language. Okay, so Ni-fi processor is the actual working component. So this is where you accompany the task. I want to listen or incoming data. I want to connect to an Http endpoint. I want to pull data from an external source. I want to connect to my database to get the data from a table I want to publish to an external data source I want to publish to Kafka. I want to publish to a table or Hdfs or whatever I want to route my data based on a criteria. I want to transform. I want to extract information. So we have a lot of lot of operation. We can do with 9 5 processor, or I think, and the installed version, maybe 306, yeah, 360 operation. We can can be done using a knife processor. So you have some examples. The get file, get Sftp route on, attribute and route on content, so get file it. As the name suggests, it will read a file or pull a file from the local system. Get Stp, Sftp, it will connect to an Sttp Sftp source, secured ftp, file transfer protocol to in to collect or to download a data file or upload a file. The route on attribute. This is, for example, you have in the header your custom attribute, or an existing already by default attribute. And you want to do routing based on the value of this attribute. For example, we can say, size the file, size or file, name, file ex the file name, you can extract the extension and say, Okay, if the extension is Json, store it into this directory. If it is Xml store it into another directory, because from the system target, maybe you need to connect to this different directory using spark and the You you are using different code different processings to process this kind of file so it can be done here. Route the Content route on content means you read the content of your data. And, for example, it is a Csv file, and you you will parse the one line or one row from your Csv file and base it on particular content to say, I want to route to this direction. So these are some examples. Yeah. So the visual command, very simple, very friendly user. It is just based on drag and drop. You can take from the toolbar the component and place them on this canva. So the interface is very simple. You can you have the status bar? The status bar will give you the information about how many processor you have? How many processor are? Stop in, start status or stop it or disabled. So you can have all this information visually from this status bar. We have 2 panels, the operate and navigate panel, so let you just navigate over your over the canva and operate which will let you start. Stop the component or process group. Just do everything with this processor. and we have the Burger menu. The Burger menu is on the top, right from here you have some access to some not advanced, but more more features in life. I will talk about this in the next class. We'll be talking about this feature. In the next class. You can view statistics, you can view, create, template. A template is nothing but a backup of your data flow. You can configure connection processor. And you can do everything visually. Okay? So now how to go the processor is can be configured. So you can right click on the processor and you will get the contextual menu. And from here you can interact, to configure, to disable, to enable to view to whatever you want to view from the processor. Now, in the processor itself you can see we have different and a lot of information provided. So let's start in the on the top from the top. So in the top we have the processor name. So the processor name. This is the name you gave to the processor. So copy, read whatever should be something has meaningful for you. Okay? So you have the status indicator on the left. So the status indicator, when you see the green triangle means it is in. It is running when it is a red square. It is stupid when it is yellow triangle. There's some errors, or you need more action to do it is not. Configuration is not complete. On the right you have the red square. This indicator, built-in indicator means you have an error. So the built-in indicator indicates this error is detected only after execution. before execution you have. The yellow triangle means something is missing in the configuration. But if you see the red square on the top right. This mean? It was an error. While executing. For, for example, I I want to connect to an Http endpoint, and I didn't provide the IP address or the I have a typo in the name of this source, and so on. So many, many reasons for that. And you will be dealing, maybe, for sure a lot with this red square. So under the name, or below, just below you, you see the type of the processor, for here you can see, for example, it is a put file. The put file say, this mean, I want to deliver this file on my local system. and the put file needs permission to be granted. Right permission put means right and right need to be to the user running. This processor need to have the permission to write the content. So this is why you can see the armor red armor on the top left. This mean, pay attention. This processor needs permission, so if you don't have permission, you will get an error. This error may be permission denied. Okay, you can see also a very small number one. This means the active task. If it is running in a cluster mode, maybe this can be more than one. and every 5 min the processor will refresh the statistics. So how many file flow ingested size, and so on. So all this information are provided by this rectangle of this processor of any processor. You have all this information for all the processor. the configuration settings again. It is just now an overview, but you will be working with this in the hands on workshop. And it will be more clear. So here we have different settings. So we need to define the We we can. It's optional, the name of the processor. The name you can give, or you can keep keeping some. Sometime. The default name might be enough, or you can just customize this. You maybe you need to schedule the running of this processor. You can define chrome. Say the pattern to to run your to schedule your processor. You need to define, maybe some properties. For example, if you are loading a file, you say, get file. Okay. But I want to read from where. So I need to define the input, directory. So this is properties you need to provide. And we need also to provide and to to maybe some documentation. It is in the comment. So all these are managed, and we can set all this information and these parameters from the configuration window. So now the connection, the connection, again, as I said, is, use it to connect 2 processor together. And this connection here you will see the number of the flow file. You can define the back pressure. You can define the threshold of the buffer. You can prioritize your your, your flow file. So in the connection it will also can act as I would say. I would say, it can be common to handle exception or failure. The processor you can connect in in case almost all the processor has an output state, for example, success, failure, so based on the task. If it was a success, you can connect the next step in the process. The data flow to connection. The connection can connect to the success. But in case of failure, for example, you want to send data to a table on emergency bms. but the you have the typo, or or maybe the connection was not possible or unavailable, and you get an error. So you you can connect and use the failure connection to retry this data insertion, for example. So this is where, in the connection, this is where you will handle error. You can maybe do retry. You can connect to yourself to do a retry. For example, I want to put data on Hdfs. But Hdfs was not available, so the data flow file will stay in the flow in the Flow File repository, enter Hdfs is available, and then it will run. So this is connected using the failure connection branch. So you see, the connection can be used as error handler, or to to just to ensure that the data flow was successful. The process group, again, as I said, is nothing but logically grouping our processor. It is just a kind. It is namespace in the namespace. We can define many, many information. You can define processor. You can define control service. We talk about shared service. This is will be tomorrow next class. So we can define, for example, Avro schema. We can define many information, the parquet, how to create parquet and whatever. So this will be the next class. So here you need to create. When you connect your process groups, you need to create input port and output port. For example, I created my data flow using 3 process group data ingestion data transformation data storage. So delta ingestion connect to the data source and get the data. Now, I want to output this data from the process group data ingestion to the data processing process group. So and then I will create or implement an output port. And in the data processing, I will implement input port, so I can connect the output to the input very simple operation just the drag and drop. And it will ask you to implement the because you can have more than one output port and input port in your process group. So you can just choose the one you want to connect. So the 95 expression language. It is now an example. It is very simple. I provided to you on the. You have the tutorial on the sandbox. and I provided to you just I think it is helpful expression, language, cheat sheet. So just this is public document on the Internet. You can use this just to remember the syntax of how to access how to do very simple things, plus, again, you have many information and more information in the tutorial on the sandbox. So now we have a particular syntax and the expression language can access the content and the attribute in the flow file. So always, always we start with the dollar sign. followed by the open, curly braces inside your function, your expression, and we finish using the closed curly brace. And this is the expression. Consider it as knife expression. So let's see an example, for example, a check for substring, matching within attributes. So I can say if the file name contains the word knife. You see, we start with the dollar sign. I have my expression inside curly braces. and so file name. This is the name of the attribute. We have the column to separate the attribute name, and to say which function I want to apply I want to use contains is a function from the expression language. I want to check. If the file name contains the word or the string, my file. it is case sensitive. So Ni-fi, in capital case or small case is not the same. Okay, so this is case sensitive. So this expression will return, true or false, if the finding contains a nice return. True, otherwise, to return my false. You can use this for routing true or false, it is binary. So you can switch using the route using this information. So, for example, reformat that you need to change the date format. Okay? So I have my string date. I read this from somewhere. This is not a default. Attribute. The default attribute is not string date. There is no string date. This is customized attribute. and the attribute file name. Here we have the capital N in the file name it is customized. It is not the default. One is, everything is low, lowercase. lowercase, small case. So string date. Here I am using the function today. This is how it is it should be written 2 dates. and I provide the date format. So here it will take this string from this attribute. and it will convert into a date having this particular format, year, month, and day. So here multivariable comparison variable, one greater than variable. 2. Anything, attribute or function should be enclosed into the dollar sign and curly braces. So, for example, append string operations. Append. Here, for example, I have an output path. and I want to append this output path, using this particular and particular name directory name. So I can use output path. which is a custom, attribute the function happened. I provide the string to upend with mathematical operation, total amount minus 5. So this will remove 5 from the total value of the total amount attribute to see very, very simple, just different syntax, but very simple. Yes, sir. Sam Wanis: Yeah, so so I see this all these operations is as you said, it's mathematical, or maybe comparison. But but where the definition or assigning of the variable is like, is there any different syntax? How to say like, for instance. total amount equal? 10. Is is there something like that, or. Khaled El Tannir, M: You will be doing this from the We have a processor. and this processor name is update attribute. You will put the name of the attribute, and you will assign the value. And this processor will be using this a lot. Sam Wanis: Oh, okay, okay. So this is done like before that in a different stage. Then then we can use the expression. Khaled El Tannir, M: I will show you this in in. Sam Wanis: Oh! Khaled El Tannir, M: The workshop. Sam Wanis: Okay. Thank you. Khaled El Tannir, M: So this is I would say, the function provided by, or the built-in function of the Nifi expression language. You can see we have Boolean spring manipulation, the encode data encode and decode searching mathematical operation and date manipulation. For example, if you can just take the Boolean logic if it is null is not null empty, greater gt. Greater than ge. Greater or equal, Lt. Less than, and so on, less or equal for the string manipulation. You can see to Upper, to lower trim. Okay. this is very important, but you will have time to practice. Don't worry. And the next class I will show you how you can use the intellisense, the intellisense tool which will show you how to get the intellisense and the documentation of each function. Don't worry just now to have an idea to give you more time to, to practice for the next week. So this is a sample flow file. So here I created it on the on the left you can see my data flow. So I have my 1st processor list. Listen, Http, so this mean I connected to an Http source, and I am waiting some something. Okay? So the processor is running because I have my green triangle just on the on the left of the name of the processor. and I have my update attribute, which is a second processor, and they are connected together, and you can see the name of the connection in the box. See? Name success, keyword 0. The name success means I am connecting this getting the data from the success connection of the Listen Http. In case of it is, the connection is succeed. I can route the data, the flow file to this connection. and then the flow file is routed to the update attribute. So the update attribute is here to to initialize any attribute value you want. and this will operate on all the attribute, even default or built in or custom, you will be using this update attribute, and on the right you can see the properties tabs from the processor configuration. Whenever you click on the processor configuration. This window will show up. And do we have different tabs, and when you select the property tabs, you will see all the properties to be set for this processor. So here you can notice we have properties on board. and some other probabilities in normal characters own the properties in bold are required. If some of them 9, 5 provide a default value. maybe it is enough, so it is enough, but sometime it, for example, when you get the Read Directory for a get file. You need to provide a directory so Directory 9 5 cannot provide you a default one. So you need to provide it. If you don't do. you will get the yellow triangle means something is missing. You can keep if it is okay for you the default value. If it is provided, or you can change it so. But all the bold properties are required. Anything involved is required. Okay, so with my file again, as as I said, we have many, many processors, and we can use all of them to to connect to different source and destinate or target destination. You can use to convert format. We can do whatever we want. So this is all I wanted to share with you. Today, after the break, we will be doing the workshop, you'll see it is very interesting, and you will learn a lot visually. And of course this is not all, because you have many things to say about, and to show you about to be able to build real time and collecting data in real time. So this will be in the next class, otherwise will be a lot of information today. Okay, so in the appendix. So here, just I wanted to show you the data programs. it can show you all the components from where it was collected, where it is gone, assiminated, and so on. And this view is not. You can. You can check it, but it's not part of the course. After the break I will show you how we can build our my 5 data flow, how we check the content, how to check the queue. Everything. So we'll do every. I do prefer to do this in the workshop instead of rather than doing this in the slide of our presentation. So do you have any question before we go into the break, we go to the break. Okay. so let's go. 10 min or 15 min of the break, and after the break we'll continue our in the workshop. Okay? So with me. Oh, cool. So for the tutorial. Here the knife high. So we have different tutorials here. So the 1st one. So you have discovering life, Apache knife processes again to summarize, and just to recall what we what I covered in in this class you have the explanation about all the interface which button is used. What what for you? This? What do you see why or what for the processors. the input ports output ports process groups. You have all the essential of 9, 5. The second, this, how to build your first, st we'll be doing this together. Okay, so how to add the process group, how to choose the processor and follow these steps. Again, you'll be doing this together. this one together. Okay, how to configure, how to run and how to see every everything. Okay. you have also this Wi-fi expression language. So this expression language will explain to you about the this, the description, the expression, language, the how to call the intellisense the data type, the structure of the language, and it will give you almost everything you need to to ingest, to manipulate, to transform your your data. Okay, so it is very important to to practice this one 4, 95, because it will be the foundation for all the workshop and homework incoming upcoming homework and workshop in the next class. In the next class, we'll be talking about the Netflix movie and TV show. We'll be doing this together because there is a lot of things to say and to you will learn from this case study. and we'll be starting, working with unstructured data, and it will be our introduction. This is for the next class. Okay? Now let me show you the interface. So from my file. So here we have. Ni-fi site one site, 2 site 2 is disabled to save memory and resources. It is not part of the course, but it is here to simulate say to simulate this distributed environment. So here we can connect 2 different instances and exchange to to a remote site, for example, so this is, but it is disabled, so you will not be able to use it, and then, if I registry is also not part of the course, but it is, use it. For let me give you the example. In in real environment, we start by creating the data flow in the development environment. And then we go to clarification, to validate if everything was good and we validate everything. and once it is done we can go to production. So here you need to version your data flow, maybe your different abro schema parquet schema, and so on. So nephr registry is used for this purpose. So we'll let you create and share your data and versioning your version, your data flow and share this between different environment. So for for me and our course, just you, you need to run the 1st one, which is site one. So when it is open, you should see something similar to this interface. So you have the main canva. and in the main camera. So here, what is? No, not once this. So we have the main canva here. Sorry. And this is the navigate panel. No, I don't want this. Okay? Okay? No problem. So in the navigate panel, when you create here, you can just center zoom in, zoom out. And in the 9 9 5 flow. Not want how to remove this. You can see this, do not know how to remove it. Okay. Now, on the top, we have our main toolbar. So processor input port, output, port and process groups remote, which is not part of the funnel template and labels. How to remove this. How can you? How can you clean this? Okay? Yes. Okay. So now let's say, I want to create our 1st link. So here, if you go to the tutorial. And if we build our 1st 95 data flow. what we'll be doing. We are reading a file. Okay, I will show you. So here we are generating synthetic data and store this synthetic data on Hdfs. Okay. so and in the workshop, you have the first.st So this is 4 part workshop. So 4 parts. And the 1st one is to let you create the to data flow. But you will connect to the local source and to local system and put a file into different directories. Okay, so we'll be creating this one. I will start with this one because it is a little bit easier. Then the one on the sandbox, because here we need to create some variable. So I will do it just after this. Okay, so 1st thing to do is to create a process group. We, I can put everything on the main canva. But but again, it is non manageable. So let's so okay, let let me start on the main cover and show you how you can put this later in the process group. So from the main toolbar drag and drop processor, and this will open the list of all processes. What I want to do is to read a file from local system. So let's say, filed in the search box, and here I will get the list filtered, and we'll scroll and seek and select the get file so you can see here it will be, it will say, great create flow files from input directory. And if it doesn't have the latest lead permission, does it matter? Okay. let me do this little bit bigger. so you can see. So you can see the navigate panels. You can. Just if you want to close this. just click on the plus sign or the minus sign. Okay? So you can see now in the get file processor, this is the default name the type of the processor with the 9 5 version, 1.2 7 a 0. We have the statistics read, write out, task nothing. It is refreshed. Every 5 min we have the yellow triangle, which means something is wrong, and when you hover you with your mouse it will indicate what is the problem to say the input directory is invalid because the input directory is required. Okay, oops. And to say also, the relationship success is invalid, because relationship success is not connected to any component. Remember. this is that flow basic programming model. This mean, any operation should operate on data and output data. So here we need to connect to to our to the output. Okay? So if you double click or right click, you'll see configure. This will open. So here. okay, I have all the 4 5 tabs. So settings, scheduling properties, relationship and comment comment is to document. So you can say, read profile room, local system. Just the documentation settings here, where you can customize the name, get file, can see. So you can see. Read my reader, file. Just give the name. Okay? So here you can just define by the built in level, which is the error level. So let's keep it by default. And now we go to schedule. It is enabled, as you can see, it is enabled. Okay. the scheduling here you can schedule, you can define scheduling strategy. It could be time driven or cron driven. Cron means you need to provide the Cron pattern, and it will be executed based on this Cron pattern. Okay. execution on all nodes. Here we have only 1 1 node primary node this is has sense when we run this in a cluster environment. But here the primary node is the only node, so doesn't make any difference here. What is important is the run schedule here means 0 means it will loop indefinitely. so it will loop indefinitely. So this you need to pay attention always to this. because this will maybe, if you connect to an Http source and you loop indefinitely like this, it you will be blacklisted. So here you can define the intervene to run this processor. So let's say, we need to to say, let's say every 60, every 60 seconds. So this means every minute. so it will run every minute or every 2 min. For example. okay, how does it have 20 seconds? So we need to pay our pay attention to this. Okay? So every 2 min or every, let's keep it every 1 min. Okay, now, in the properties, we have all the properties listed here and again, everything in bold is required. Some of these are already, if I provide a default value for others. No, for example, for the input Directory, there is no value. So you need to provide this value. The filter this is a regular expression means take everything so we can define here, maybe to to just read particular name of file, or just define the pattern. You can set the name of the file so you can do whatever you want here. The batch size. How many file to read in a single operation for it. Here it is defined to 10 by default. 10. So if in this directory, in the input Directory. We have 15 or 20 file files here it will be. We need 2 operation to read, so one, the 1st one will read 10 files, and the second one will read 10 files. Okay? And a keep source. So let's say, here, I want to define my input directory. So I am reading from my local system. So home training data, it is case sensitive. Let's say I want to read my 5,000 hyphen. 8. Text. Oh, sorry, not this one. No, I created already one for 9, 5. So let's say, okay, 9, 5 in. you have already one created for you. Okay? Because I wanted to show you the, how can we put this file later? Okay? And here I want to read. I don't want to read anything I want to read. Only this file. Okay? Okay, batch size. We have only one file. So this is important. Also keep file source. After reading how we want Wi-fi to behave to keep the source or to delete it. For instance, let's say I want to keep it. Okay. great. And everything else should be enough for us, for for for this flow file. Now for the relationship you can see here success terminate or retry. Remember, this is flow, basic programming model. So if nothing to to be processed or routed. After this processor, we need to explicitly terminate. So it is not the case, because I want to store this into another directory. Okay, so let's keep it open. And now let's create, add a new processor. Let's say, put file. Okay. So you see. So I still here have my yellow triangle. And as you can see, it's a relationship. Success is invalid because relationship success is not connected great. We will connect this in a moment. So you just remove this. Okay? Now the put file, the same. I have the little angle. And here I the settings right. I'm fine. Okay. so scheduling here we are reading the the content from a flow file. It is not reading from an external source, so I can keep the run schedule, as is no, no problem for the property. Now, I have the to provide the directory, which is the output directory. Okay? So I will say, whom training and Wi-fi out. Okay, I use it in and out. Well. conflicts, resolution, strategy. What this mean? This mean in case of the file already exist in this Directory, what? How should I behave, replace, ignore, or fail fail means it will raise an error. And you should catch this error in the in the data flow. Okay, ignore, it's ignore, replace, it will replace. Okay? So we can say here, ignore. create, missing directory. Yes. So this mean the output directory. If it doesn't exist. Now if I will create it. But remember, if you go to settings here, not no, you cannot see this here. Yeah, here you have the armor. The Red Arrow means this processor needs permission to be granted, even read and write. So the for these directories we granted this permission to to 95, and if it is not the case, I will give you the the comment to grant permission to Ni-fi to to be able to write down on the local system. Okay, so scheduling, it will run whenever I receive the flow files, properties I so here I can define if I need the permission owner and group. So. But the user, I think he's not. Does not the permission to do this? Okay, the relationship? Do I need to do something else after this processor? No. So I came here, and I explicitly say, in case of failure, I will. I don't want to do anything in case of success. I will not do anything anymore. So I will terminate the flow file. So this mean, if it is not done, means you need to continue the flow. But our floor terminates here. So we need to terminate this explicitly great. Now, what we need is to connect our processor together and create this connection. You see in the middle. You have this green arrow just drag and drop to the to this processor, where you want to connect to, and you will see the green rectangle by releasing the mouse, it will open the create connection window, and here, by default, it will select the the relation connection. So if you have more than one, it will not be selected. But if there is only one, Nifi will selected. So you can see here in the details. So we are connecting on the success relationship. Okay, no problem. And the settings. For now we give this as is so we can give a name to this connection. We can define flow file expiration. So flow file expiration means, if you say, after 10 seconds all the content will be deleted. After 10 seconds. I use this workaround to create a kind of trash. because in 95 we don't have the concept of a trash. So if I don't want something flow file, I want to delete it. What I do I create a connection, and I define flow, file expiration period. After this period the profile is deleted. and here you can see we have the back pressure. So the back pressure object here we can handle up to 10,000, which is enough. We are working with only one, for instance, and the size is 1 GB. So we have enough. Before this, the back pressure mechanism will be triggered. the load balance strategy. So here we can use different load balancing strategy. Do not, for instance, we don't use this, but in case of we can use road, robin, single node. And so again, this has sense when you are in cluster mode. Okay? And here we have the prioritizers. 1st in 1st out, newest flow, oldest flow priority attribute. I will show you. Maybe this priority attribute how it works. It is the easiest one. Okay, but, for instance, let's keep, as is by default. and just ensure that the success relationship is checked and click on add. Oh, now I have my connection created, and as you can notice. the 2 yellow triangle has gone, and now we have 2 red square. This mean my flow file is correct. Nothing is missing from the configuration side. Maybe when you run, when you start you get errors, but it is not related to the configuration. Okay, maybe something else, syntax, typo, or whatever. And now it they are in. Stop it. Mode because I have my red square on the status bar. You can see I have 2 processors here, and the 2 processors say we have the square red square. This means 2 processors are in. Stop, stop it. So now I can run individually, each processor. I can run all of them in a single click. So if I want to run them individually, which I will be doing now just right. Click on each processor and say, run once. okay, to run. Once. start, it will enable and start this processor. And until I click on, stop. If I want to run all the processors I choose any empty place on the canva right click and start. This will start everything from from the operate panel. You can do the same. You can select one processor and run, or if it is, you click it on an empty space on the canva. It will run all the processor in a single operation. Okay, good. So now let's check on our hmm. okay. So now I have my mobile external delivery training. Okay? And from the training you can see, we have the directory. I cannot make this little bigger. No, I cannot. So on the panel you can see that I have my, in which, for instance, it is empty. Okay? And now, if I out. So wi-fi out, I can delete it. No problem. It will be created because I asked it to do. Okay. So now I will. I just want to put a file inside this, I can do copy turning the data, 5,002 whom? Meaning 9, 5 dude. Okay, if I refresh. Now I have one file in our directory. Great! No, if I start this processor, it will read, and as I defined here every minute, so it will check the content of this directory every minute. Okay, for instance, let's do write once, run once. Okay, so you can see it was very fast the task here I had this small one to say, I have one task in the back. in the, in the server running, and now I have one file one element out. One element. The size is 1.3 megabyte in the sixes. I have now the queue. which is 1 36 MB. Now, if you click right click on the queue, you can see list queue. So here we have different elements in the in this panel. So the problem here is, this is very large. Let me choose another file. I want to, because I want to show you the content. This is large. Okay, so I will empty this, and it's say, another one. Okay. let's say, I think we have the hadoop. I have a small one. The name is data. I think it is hadoop something like that. HHHH hadoop. MMM. BBH. Hadoop text. Okay, we'll take this one called this small one. Okay. okay. okay. I didn't use any filter, because I want to be sure, I'm reading only this file. Even I have different files. It will read only this file. Okay, so let's run this again. Okay, so I have now one file in the queue and let me list the content of this queue. And here you have the. You can download it download this file. You can view its content, or you can check the data province. Okay, so for the content. It will say, Okay, this is the content of our file. You can see it is on the top. Right? You can see, this is a plain text. Okay, this is plain text. If it was Json you can choose formatted, it will beautify the Json for you. Okay, so from here you can see that we have position position one. This is the position in the queue. Okay, the uuid attributed or assigned by by 9 5 to this element, then file name is hadoop, the size, queue, duration, and lineage duration, penalize it or not, it is not penalize it, for now, so here you can download, or you can view from the left. If you click on the view details. you can see here more information. So you can see the default attribute and some details. So you have the uid, the file, name the file size, the queue position, queue, duration, and attributes. You can see the absolute pass from where the this file has been loaded. File incarceration, time, group, file, owner, permission. You see, all these are built in attributes. All the attributes, when it is shown in this window are sorted alphabetically. So this is why I recommend always, when you create your attribute, if you have more than one, just use a prefix to. So this way all your attribute will be sorted and group it together. Otherwise they will be in the list, because, as you can see, A, F. FFFF and file file path. And you all the attributes are sorted alphabetically. Okay, so now we have our in the queue. Now, I can run this processor. Okay, run this once. Nothing happened as an error. And now I can go back to my window as such, and if I do a refresh I can see, I have my Ni-fi out Directory created. And I have my hadoop text inside. So now in the input, I still have my input file. So if I go here and say configure. Now keep source file false. Okay, run this again, and I use the conflict strategy, ignore, if if already exist. Now, if I read this again, read once, okay. doing good. Okay. I have my file in the queue. and I can run this once. The queue is emptied. Now, if I go back to my directory. So in the input, if I refresh, the file no longer exists. it was deleted. This is what we want. Great. Okay? So now our 1st our 1st data flow is working is complete. Now it I. As you can see, I do not have create new. So I need to organize this into pro process groups. So let me create my process group. Say. bye, boost data flow. Add, okay. Now, you can select oops. Sorry you need to hold the shift key. and you see, drag and drop. The blue rectangle means it can be moved into this process group. So if I double click. you can center it here, or you can just click on the mouse and move the on the camera. So you see, now, I have my process data flow inside my process group. Okay? Now look at the bottom. In this, in the, in the bottom, you have here the navigation. When you have the 9 5 flow level means you are at the main level main camera. When you enter, you can enter by doubly clicking or just right click and enter group. Okay? And you see now the navigation panel and I am inside the my 1st data flow process group. Okay? So you can navigate here. You go back to to the name or enter to if I have more than one, it will be in the bridge comp here, so it will be listed here. So from here you can navigate back to the main camera. So from here you can say, Okay, now, I want to create another process group and so on. Now you finish it. Your work and you want to create a backup. How can you do it? This is what we call a template. And this is what I provided to you on my courses. Okay, so right click. And now you can see, create, template when you see, create templates. So this mean, okay, this is my 1st data flow. Okay, you can give a description copy 25 wrong local system. Okay, or move a file from local system. Now your template is can be downloaded from the Burger menu. From the Burger menu can see template. And now this is your copy and just here do download so it will download the template. Okay, it is an Xmn file. and this is from the next, not the today. Homework. But the homework? 3, the individual number 3. You will be providing me. And if I flow this way. okay, now, how can you upload the template I provided to you. So it is very simple. You can say, upload right? Click on the canva upload template. Okay, you can select the template. For example, this one. Okay, let's click this one. Okay, upload. Okay, the template successfully imported. And now you can from the template icon in the menu bar, just drag and drop and select the one you want to put. Okay, so the workshop 7 and click. Add. Now you have the template I provided to you in on my courses, and it is exactly the same. So now you can double click. and you have access to the the content. So you have the local file copy the 1st one. It is exactly what we did, this one. So get a file and put a file. The get a file is from the input directory. So here I am taking any file, and if any file you put in this directory will be loaded, and, as you can see. Keep source file false. the scheduling 1 min, 16 second, 1 min. It is exactly the same. Okay, yes, Sissa. Sam Wanis: Yes, doctor, thank you. So 2 2 quick questions. So for the file like, when we specified the file name, and we like we would like to have all the files to be copied. I expected this to be asterisk dot asterisk, so I see. Is it a regular expression that was used to for the file name dot asterisk. What what was this Khaled El Tannir, M: It's windows. This is not windows. Sam Wanis: Okay, so this is the way we define it in Unix or Linux. So by by having these brackets and. Khaled El Tannir, M: Explicit. Sam Wanis: Regular expression. Yeah, okay? And the second question is, for the templates. this is the way we can save whatever we have done like if I if I need to open next time. and I just you know, load what I I done. I need to keep it for another session by not saving it to my local computer, but by storing it on the version machine. Is there any way to do it like save this template, or whatever configuration I did to to continue work next time. Khaled El Tannir, M: Let me understand the question. So you create your data flow. And you created you just click, right, click and say, create template. And now the template is saved in the template menu. If you didn't download it, it's still. Sam Wanis: Oh, okay. Khaled El Tannir, M: So it doesn't make any sense. Sam Wanis: So. But okay, so if I open next time, like, if I you know shut shut down, or or terminate the session, and close the computer. Open it again without the saving it to my local. Will it be there? Will it be stored Khaled El Tannir, M: Next time. The next time you shut down the sandbox and you open it by phone. Everything. Sam Wanis: We'll save everything right. Khaled El Tannir, M: Yeah, everything is here. Sam Wanis: Okay, so, yeah, so. Khaled El Tannir, M: Upload the template again. It is! It will be. Stay here. Sam Wanis: Okay. Okay. So what you have shown to us is only was only to show to us how to save it. But it doesn't mean that it is a required step to to keep it. Khaled El Tannir, M: To keep it for next session anytime, every time you need to open you open. Ni-fi. Everything you did is here. Sam Wanis: Okay, okay, so this is only to maybe the Xml file to send it to you or to pass it to another person, but is not for open it again. Okay, thank you. Khaled El Tannir, M: Correct you can, if if you maybe, if you do something and you delete your template, and now I can. For example, I can delete it I can. It is no more on the canva, but it's still in the template. So here I can do again. For for example, here I just manipulated this. And let's say I modify this, and I don't know how. Okay. let's say I put it something else. And now I have an error. But I don't know how to go back, and whatever what I can do with the template, if it is, for example, as here, I can just delete it and get it again, and it will be as is. see, so it can be used as a backup local backup until you decide. Of course, if you say, create template again, so it will replace this one. Sam Wanis: Yeah, and. Khaled El Tannir, M: Now, if you want to save or create your backup for template, you can just download it to your local machine, to your host machine. Sam Wanis: Okay. Thank you. Khaled El Tannir, M: Now let's take a look to this. This second part. Second, yeah, the working with process groups and routing files. So the 1st one is. Now, you have an idea how to create processor, how to configure processor, how to connect them together. Using this connection. Now, while working with process groups. as I said, we need to connect them using input and output port. Okay? So this one is very simple. I I just split it. The input, the the 1st data flow. So now I have get file. Okay, I see it. Nothing changed. Here. Keep the source file, for instance. It is true. Okay, and it only one processor and the output instead. Putting this on local system, I put this on Hdfs, okay, so let's see this, how it works. Now, as I have 2 process groups, I need to connect them using input and output port. So when to create an output port, so just output and give a name output, one output, one, for example. So here, local or remote, of course, in our case it will be always local connection. Click, add, and then you can just drag and drop. No, don't want, because we have already one. Okay. why don't want don't don't want. Okay? No, it is enabled. Okay. No problem, because you have already one in this case. Okay. hmm, Steve, yeah, the the later angle means it is invalid, because we need to connect to something. And now I can connect, and it will ask me to connect this together. Okay. now, the this is normal, because the connection is, stop it from the second processor. I will do the file. I will create a input. Port. Okay. this is the oh, sorry. This is the output. Oh, I know why the problem was. okay. This is because I use it as an input, port. Okay, output one, okay, and then connect. Now, I can connect. I have 2 different output doesn't make sense. But when you see this yellow triangle, these mean because you didn't connect this to another process group, which is normal. Okay, so let me remove this. And from okay. So here I am creating the input and adding to the processor. Now, as let me remove this so I can show you. So now I have 2 process groups. I can connect them the same way. And here it will ask me the from output to input so from output, I have only one output files out and I have only one input in the destination. So it it is only the single element in the list. Add, so now, as you can see, I have 2 red square means it is stop it. And now turn it into Red Square instead of the yellow triangle. Now let's run this and see how it. Okay, this is a new to put Hdfs. This is new so it will. Usually we don't need this. You don't need this. Okay, so here it is exactly the same. But instead, creating on the local system, it will create on Hdfs. So the Directory here, I will say, workshop 9. 5. Activity to out. Okay, in case of it already exists, you can do, replace, ignore, fail, or open. So let's say, replace. Okay. writing strategy. Write and rename. Okay. But we are using Re to replace. So no problem here, keep everything by default. It is not required. So block size. Everything is by code and compression. Codec, we don't use any compression, for instance, but we could use different snappy and Bz Gz default. But, for instance, keep it as a raw text. Okay. the Hadoop configuration resource, as you can see, it is not in bold, but sometime in case of knife is not able to find or to connect to to the Hdfs file system. We can provide the location of this configuration file, so clarify can read directly from this location for. But you didn't. You don't need that. So just let's start and see how it works. Okay, I can run from here. If I do start, it will start all the processor inside this process group. Okay, let me check if we have file. So it is input and keep source file. okay, and run every 1 min. Okay, so now it is empty. Remember I it is empty. So let me put. And here it is running. It is running. I can do a refresh. We have nothing. Okay. So let me put a file. Okay, and if I do a refresh it will run. And I need to wait 1 min after 1 min it will be loaded because we put 20 min. So let me, I'll stop and run once. Okay, run once, okay, I have my file. and it is stop it it wait that my file output port is open. So let me start this. And now you can see the queue is empty, so it is waiting on the other. So here you can see in the queue. It is waiting between the input processors and output processor. the from the input side is green. So it is open. It started. And from the output processor still red. So it is stopping. Okay? So if I double click and I run this with the start. okay, refresh. And now you can see the file is cured and wait for the Hdfs part. And now you can see we have both green triangle. Okay. So now from the Hdfs again, as you can see, the relationship here is explicitly terminated because we don't have anything to do after this processor. So I explicitly terminate the flow. So let's say, run once or start doesn't matter. Okay, do a refresh. Okay, it was. The queue is empty and there is no task running. So what I can do now I can go back to my Hdfs. So what was it? Was this workshop? 9, 5, 8, 2. Okay. So browser. Workshop 9, 5, 8, 2 out. And I have my file. See? We start doing something beautiful and amazing. So we can stop. So everything is you can see on the top. We have 2 processor running. We have 11. Stop it so I can just right click on any empty space, click on, stop, or from the panel. If you click on the stop it will stop everything all the running processes. So say, stop. if you do a refresh. You see, now we have 0 processor running great. This is cool. So you have all this explanation in the workshop, so part one is just to explore the part 2 is creating the the first.st So here you have, I guided. I guide you over all this configuration and settings. so I recommend to do it by yourself, not only to to just, to to read and just play with the template already I provided to you. Try to do it. The second one is for working with process groups. This is what we are doing now. and the latest one with this part is routing. So in this part I will create a routing strategy. And this routing strategy is to route the file based on their extension. So the idea here is, I read, a file. If the file is, has the extension, Json. I will route this to a particular directory and store it on Hdfs. If it is Xml. I will route this to another directory and store this on Hdfs. So here I am using the in this exercise workshop. You can see I am using almost the same processor. For instance, we use it, get file, put file and put Hdfs. So all these are common now very common. The new one is the read file name extension. To to route to Update the attribute. Because I want to route to the output directory and I want to be dynamic. I want this to be dynamic. So let's get inside this configuration setting and see what is how it's going. So here, I'm reading from the same input directory. Okay, so it is home training. I am reading everything. Okay. So any file here we can read and route. Okay, all the others. So keep source file false. This means it will be deleted after being read. Okay. So records Directory, we don't maybe don't need. But okay, let's do it to false. But it doesn't matter. I don't have any subdirectories. Okay, so this is nothing new here, and the scheduling, which is the important part. It will run every 5 seconds. Okay, just to. I don't want to wait a lot. Okay, now, let's put 2, 3 files in this. So I have my input. Okay, I have my hadoop and I will put my, I have home dream data. I think I have books. And Mixml, okay. wait Wi-fi in. Okay? And I think I have a transaction in Json transaction. No, maybe one. - I don't remember. But we have one transaction. Let me check the name of the file. Yeah. Transaction transaction one. Okay, we can take any of these transaction. Jason is very big. Let's take the smallest one. Okay, transaction one. Okay. Okay. G, 7 and 9, 5 great. So now, if I go back to my my file directory. So I have 3 files. I have my books. Xml, I have my hadoop text, and I have my transaction. Json. okay, so now I want to route. base it on the extension of the file. But here I created only 2 output, 2 directories for G 7 and Xml. If it is not G. 7, and it is not Xml. I will route this to the trash. I don't want to use it. I want to delete it. Okay. So now let's read, we can do start once, and, as you can see in the property, it will read 10 file as a batch. Okay, if I put one, it will read only one here. If when I will start this processor, it will read everything in the 3 in the single operation. Okay. run once. Now, as you can see in the in the connection queue, I have my 3 elements list queue. And now we can see we have my hadoop text. We have my transaction. Json and I have my books. Xml, okay, we don't worry about the content or whatever. So just, we are focusing on this exercise and the extension, because I want to route, base it on the extension. Okay? Great. So now I am using the route on attribute great. So let's see how it works. So the route on attribute, as the name suggests. I am using the attributes to as a condition to route. Okay? So now let's start by the setting setting. This is the the type route on attribute. Okay, this mean, it pays it on the criteria function. If it is true or false. I will route or not. The name is customized file name extension, scheduling. It will run whenever the data flow is here. We are not reading from the external I am reading from an internal processor, so I don't need to to schedule here the property now here something very special. These are not by default, because when I drag and drop the processor route on attribute, and I added, as you can see, it is empty from here, I need to to add the condition for each attribute for each attribute as I am routing on attribute, so I need to provide the attribute, name. and the condition to to use to to route on this basis on this value. Okay, so I will copy paste. And I will explain to you. So here I need to check if the extension is Jason or is Xable. So let's say the property name. I want something that I understand when I read has meaning, so I will say, is Jesus. It's a name I give you can give any name XYZ. Whatever, but just to to to remember what it is. And now I need to pro. So how I did it, I click on the plus sign to add these attributes. Okay, is Jason. This is the name of the attribute. Okay. And now it will open this window. And this is where I need to provide a function that return true or false. So. Remember, this is where I need. When I need to manipulate attributes or content, I need to use the Wi-fi expression language. So an Ifi expression language start with the dollar sign. and it is in 2 curly braces. So I start with my dollar sign, curly braces file name. This is the built in attribute. It is in lowercase. So use this built in attributes. Remember, we have this column file name in the when you open the queue list, and you see in the in the column you have the file name. So now, substring, use the function substring after the dot that separate the call, the file name from the extension after so substring after the dot equals. So him I am chaining 2 function. The 1st is substring, after the second is equals, so file name substring after equals Json. This will return, true or false. And this is case sensitive. Okay, if it. If the string after the.is G. 7 it will return. True. if it is not Json, it will return to to the relationship. Here we can see unmatched. If it is something else, it will route it to unmatch it. Okay. And now I will add the second one, for Xml is, thanks a man. So it is exactly the same. But here I am changing the condition and say, instead of Jason, I will say, excellent. Okay. So now I have 2 attributes. I can do apply. Okay, I still have this yellow triangle, which is normal, for now, because it is not connected to any other processor. But you can see now I have added 2 properties is Json is Xml. And in the relationship now I have I by default, I had only unmatched. But now I have is Json, and is Xml. Because I, if I add his text. And here I put, if I name extension text and do apply the relationship, apply. Okay, apply relationship. You see, I now I have a new one instax. So whenever you add attributes, it will add this as a relationship great. So now we need. So this is exactly what I did here. So I have added, my 2 attribute is G. 7, I added. My criteria function that return true or false. And now I didn't include any text, because I have here the unmatched. which I'll I'll be using to to to delete anything else. So no, for the unmatch it. So here I have my connection. Trash is connected to unmatched okay settings. If I go to settings I give a custom name trash. and this, as you can see here I gave, and I set the flow file expiration. So after 10 seconds it will be deleted from the queue. If it is something else text, it will be deleted 10 seconds after. And now, as I have, my route is Jason, and is Xml. I created 2 routes. So the funnel here. This is the funnel. The funnel is very useful because it will let you terminate a connection or data flow so as you can. Or can you, you can use this to combine data flows so as you can see here, I don't want to do anything else after this. Read file, if it is text or whatever. So I just connect it to a funnel, and here it will stay 10 seconds, and it will be deleted in. Now, if Jason, I connect to an update attribute. See, it is an update attribute. So here I will create added a new attribute, or I can set built in. If I can change the value if I need to change the value. Okay. So in the property again, this is not by default. So let me show you. So here, I add, update attribute. Okay, and, as you can see in the property I have the built in. But I do not have any custom. So here I can use the plus sign. Let's say, file path. Okay, this is the name of the attribute. And now I can provide value. When it is Jason. I want to write into this directory workshop. 9, 5, activity. 3. Jason, okay. And when I apply. nothing happened because this is just update attribute. Okay in the relationship, for now it is yellow, because it is not connected. But this is what I did here. So if so, I added the file path and I defined the output directory. I want to route to a workshop, Jason. the same for the Xml. But instead, Json, I will output to the Xml directly. No. I want to use this, the output to create the Hdfs. So here I use the funnel to combine the route. The data flow to be routed, both should be routed to to this output. And now look at this magic here. So here I have this again. It is not needed, because knife in the sandbox can find the Hdfs configuration. So in case you have error, I can provide you this configuration file location. So now you can see in the Directory. I didn't use a static or hard coded the output directory. I use it. The value or the content of the attribute file pass okay. In case of the file already exists, the strategy is to replace, and this is no compression. But again, the file path is here taken from the update app attribute fire. It's okay. So let's now run. Do run one run. Okay, run once refresh. So it was the text. So as you can see in the in the list queue. We have the file name text. So it was routed because it doesn't match text. It is not Xml. It is not Json, so it is unmatched connection. So here, if I were 10 seconds, as you can see now, it is gone. It was deleted from the queue. Okay, so let's read. I have 2 more in the queue. So let's say, run once. Okay. And this one was Jason, because I in the queue, I have you so I can run it again. No problem. It will go to the other run once. you see. Now I have my G. 7 and my Xml. What we are doing now is to set the output path. Okay, so let's check this run once. Okay? So now it is here it is. Now I can go. I do check. So it is the G. 7. Again, I can show the content formatted. You can see on the top right. Application G. 7. Now we can see this as a formatted or as original formatted. Okay. And now, from the attributes, remember, the attributes are sorted alphabetically. So I, my my attribute file path is set to workshop Wi-fi Json and the crowd and attribute to us is Jason. Okay? So everything is fine for me here. and I can now route the second one. Okay. So if I go to the queue, I can check the Xml. The attribute here file path is workshop 9 file xml. And it was routed from the is Xml great. This is what I want. So now, if I run once they put Hdfs, it will take one element from the queue. If I do start, it will take everything. So let's go to start. Okay, it is it? Keep running. So okay, I will stop it. And now let's go to our directory. And now you can see I have my a 3 created. I have 2 directory created. As you can see, we have the D flag, which means it is a directory, and the size is 0 and click on. You have our reason, and we have our Xml. So it works. See, we can do amazing things. So this is so, let me stop everything. So this is the basics, for for for now the Wi-fi, so in the next class we will continue, but we will move one level up, so we'll be talking about shared services, and these shared services will let you connect to a database to stp source, and so on. So and create stima for Avro. Read Json. Read Csv. All this kind of operation cannot be done. I will say, with this easy processor we need to complete with many things other. And this will be our next class, and also the next class we start working with unstructured data. And this answer to the data, I will. You will learn how we can process and simplify and work with Json because it again. It is easier if I take a raw text, it will be hard, because we need to go very deep into the coding to be able to extract something from this raw text. Xml is harder. G. 7 is easier, but you will see we will be doing things very, very powerful things. So today. Do you have any question about this? So this is the cheat sheet I provided to you on my courses. It is available publicly on Internet. So you can. You can check if there is a new version. But I didn't. I didn't think so. This is just a refresher about using the operator and the syntax. But again, you have this tutorial. It is very important to to let me do this big, long one. so it is very important to to do this first, st I would say, hands on, and this is very important to to to read, and maybe to practice a little bit with an I-fi expression language, because it is the basics for your 2 homeworks upcoming homeworks. Okay? So now everything I did. It is in the document of the instruction of the workshop. 7. Everything is here. So if you need to remember or recall something, you have everything here. Now, today you have your second individual homework. which is a build that ingestion process end to end. The very simple. You have all the pieces you don't need to go to chat, Gpt, or whatever have all the pieces just need to connect them together. So here you are. We, we are processing real data. This data is not auto generated. It is real data. And I choose to provide you this data set because you will notice by yourself how it is difficult to get data clean here the data is correct, but we have a lot of errors. And I wanted you just to be in the real situation. real world situation, where we have to deal with with these errors to clean, and so on. So the data is from the New York City Taxi and Limousine Commission, Tlc. And your task is to to do all the process end to end. So you ingest data. Ni-fi, is not part of the of this homework. It is all you. We covered it until last week last class. So the data ingestion data preparation data storage and not is not snappy. This is an error, a typo. It is not doesn't matter. The data analysis and data visualization. Okay. so here, I provide you. Yeah, the the column name and column explanation are provided in a separated file this file was provided by the Tlc. Company, so it is not from me. It is from the Tlc. Company. and I provided to you, attach it with the instruction of this home. So this is an example of the output. The column name. I didn't change anything in this example, data sample. The second file I provided you have to use is the lookup data. It has only 4 columns, location, Id borrow, zone and service zone. And this is the output sample. What you need to do. The data is provided in parquet. You didn't use Csv, it is provided in parquet. So you need to load this parquet file using spark. do some cleansing, filtering output again in parquet format partition it create your schema in hive. Do some query, using Trino and build the dashboard. Okay? So the data again, data is provided in parquet, of course, except this lookup data, because it's very small table, very small file, and when it is very small we don't need to to create a parquet. We just keep it as is as a text. Csv file. But you need to do some cleansing. Okay, spark Scala, do some filtering, some name, rename collection Colon, whatever you do. Partitioning in parquet because we need to keep the original format create the schema in hive. Do some write some query in Trino and connect to a dashboard superset? Okay? So the data is located in the sandbox in this home training data, New York data and New York lookup the table. So these are 2 different directories. You have 6 file in this directory and one file in this directory. So what the task to do, the data preparation here you upload data to Hdfs report the size of the rights that are stored in per k format on Hdfs. So report the size on Hdfs. Retrieve the schema and compression codec from the metadata of one per Ket, file if you can, you can take anyone, because all of these are the same. And when you extract the schema, I just provide you this Grep function. This will let you extract or isolate the Codec name compression. Codec uses for this data. You need to combine this with the command to read the schema from the parquet. So the data normalization. So here, where we'll be doing some filtering, some cleansing. So here we have more than one. It's supposed to be for 3 vendors, but we have more. It is errors. So we need to filter and keep only vendors. One and 2 remove records outside the valid data frame time frame. So April 24th to June 24, th because we have 6 months, I think. validate and clean data by removing record where passenger count is 0 or negative, because we have records with 0 passenger is yeah, it is possible, because maybe, the taxi returned with no passenger. This is possible, but negative. It is not possible excluding trips, trip distance less than one mile, because here we have a lot of errors. We have sometimes 300 or 400 miles. Well, anyway, so just keep this threshold, let's say 100. So anything should not exceed 100 miles. Ensure the fair amount and total amount are positive values because we have negative negative value. This means the driver gave money to the passenger. So this is not real. So generate a summary report, contain Count mean, and Max for this column the partition strategy. We need to partition the output by vendor, id year and month. I recommend you to use always, always everywhere. Lower case. Don't use uppercase. Please use always lowercase. Otherwise you will be hitting some problem with casing. It doesn't work. You don't know why. Use always lowercase. Now store the processor data on Hdfs in parquet because it is already in parquet. But here we need to keep the original compression codec. so from here you will get the codec. What is it? What is used here? So here you, you retrieve the name of the compression codec, and you need to keep it as is. So you will output parquet and use the original compression. Codec. from here. All you need to do for this part section is to report the total number of answers of the clean data set and display the 1st 5 row from the cleaned data set for the lookup. This is the second table. So here we have double quote. and just remove this double code. When you read, when you load this file, remove the double code and save it on Gfs and keep the Csv format without the header. Okay? So when you use it in hive, you don't need to exclude the header. because, remember, in hive excluding the header is only logical to hive other tools will process the header if it exists. So you need to remove the header. Now for the data analysis. So you have 8 query. some of simple. So the count of the number of trip identify which month has the highest average speed. The average fare per trip. Average fare per passenger. Yes, some. Sam Wanis: Yeah. Sorry to interrupt as a last point for lookup data preparation. So it's it's required to The raw data contains double quoted, so it's required to remove the double code, and also to remove without the header, and so to to save without the header. So the question is, this is something we need to do manually with the Ccv. Or Khaled El Tannir, M: No, you have all the instruction. You have the option. You have one option. so I can you, just when you read the file in spark. Just add the option with the limiter. It will. Sam Wanis: Okay. Khaled El Tannir, M: Oh no! Sam Wanis: Okay, okay, got it. Okay. Thank you. Khaled El Tannir, M: So now all the class has the the answer. use the delimiter and the option in the header. And when you read. okay, so a little bit more, not advanced, but just different. Query identify the preferred payment. Method is the shortest and longest trip distance. Borrow with highest and pickup, and drop off and percentage of pickup for the percentage. I don't accept. Hard coded value should be dynamic, calculated. You have the example in in the historials. You just check, and we will get the answer for this. query, the data visualization. So you need to create the dashboard. Just use whatever you want as chart should be just related to, to the, to let answer, and and understand what we are reading as a chart. So the total trip per month, the total passenger per month, the average daily income and distribution by passenger count and average fare per per mile. This should not take you more than 3 to 4 h. because usually I used to do this in class before. When I went. I give this class discourse in the in class, in classroom. We, my student, do not the same topic, but very similar. We did this in class. So you should be able to finish this in 3 to 4 h. not more. Maybe, if you are not very comfortable with sequel. Maybe here, maybe in this. Only this part need you to to a little bit think about it, but all everything else. Very, very easy, very easy. And you have everything. Just check this case. Study. I provided to you the tutorial and the exercise in the workshop. You have all the pieces. You don't need to go and search on the Internet environment what to submit very similar to the previous homework. So the word document, with your full name and just copy. You can copy paste from your Zeppelin, but should be readable. Okay, not if you do a very small. I will not be able to read, maybe, but should be readable, and your Zeppelin notebook, and an export of your dashboard, so to remember in superset to export a dashboard. Know where it is. We go to quick ink superset. Let's take this one, for example. so you can see here you see download export as Pdf, or as an image. Okay, so you can put this into your word document, or just export to Pdf and post it. Post this Pdf. On my courses on the submission. And if you do something special, I don't know. So here, for example, let's take this one. Just do copy a screenshot of this, so I can check your your configuration for this shark, for example. And if you create or use any saved query. Just provide. Because if I I'm not the only grader, the the grader should be able to to to evaluate all your work. So if he and maybe sometime in the last session, the grader, if you didn't provide anything, everything you are penalized. So I don't want you to be penalized for this again, the no submission or 0 withdraw 0 penalty. If you are submitting late, do not submit late. Okay, if it is 1 min, maybe I will not penalize you. But please don't do it. And again, don't use. We, we know when you are using Chat Gpt, or whatever robot to generate your code. We we know. so don't do it, please. If you have, you are here to learn. This is what I wanted you to learn. So again it is. You have everything here. Don't don't need to go anywhere else to, to chat, gpt, or wherever. and if you have a question about not how to code or how to debug your call. But if you have a question about this. the submission, just ask, ask. It can send me an email, and you can ask me. So do you have any question? I I gave it to you the answer of how to remove the double code. This is a bonus for all the class. Okay? So if you don't have any question, I will wish you a good luck. Very good night, and see you. The next class. The next class will be continuing with the Wi-fi and explore our movie title with data, flow with Netflix. And we start working with unstructured data. And Json particularly. Sam Wanis: Yeah, sorry, doctor. I'm I'm really sorry. A quick question. If it happens that by mistake the virtual machine is closed, be before shutting down or before closing it, how to fix it. Khaled El Tannir, M: I don't understand the question. Sam Wanis: So. So the window, the window for the Oracle Virtual box you mentioned that we need to first.st Stop it or pause, I mean. stop it then, or shut, shut down, and then before close it, otherwise it will cause. So if it happened that by mistake the window is closed, how to shut it down. Khaled El Tannir, M: Okay. So let me just understand the question. So here are virtual box. And this is, let me check also. And this is the window for the. Sam Wanis: Not not this window, the window for the main application, the main application, the oracle, virtual. Khaled El Tannir, M: This one. Sam Wanis: Yes, the main window. Yes, if it is by mistake closed like I closed it for this one from here. Yeah, yeah. Khaled El Tannir, M: Because. Sam Wanis: Okay, how to shut it down. Khaled El Tannir, M: Your virtual machine will still running. Sam Wanis: Yeah. So how to shut it down. Shall I open it again? Or. Khaled El Tannir, M: Okay, if no, no, you can open it again. You can open it again, or you can, for we can shut down this virtual box from. If it is open, just click on the click and send a shutdown signal. Sam Wanis: Oh, okay, okay, that's clear. Now. Okay, thank you. Khaled El Tannir, M: Welcome any other question. Okay? So I wish you good luck. Good luck, and see you next time. Cristal Cortez: Thank you. Bye. Faïçal Sawadogo: Thank you. Thank thank you. White. Rajesh Kamaraj: Thank you, Tanir. Have a good weekend. Khaled El Tannir, M: Have a good weekend. Thank you.